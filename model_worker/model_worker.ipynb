{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "from api_models import get_models, ModelListResponse,ModelPermission,Model\n",
    "response: ModelListResponse = await get_models()\n",
    "model: Model = response.data[0]\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import asyncio\n",
    "from openai_streaming import process_response\n",
    "from typing import AsyncGenerator\n",
    "import time\n",
    "# Initialize API key\n",
    "openai.api_key = \"EMPTY\"\n",
    "openai.api_base = \"http://127.0.0.1:8100/v1\"\n",
    "from api_models import get_models, ModelListResponse, Model\n",
    "response: ModelListResponse = await get_models()\n",
    "model: Model = response.data[0]\n",
    "\n",
    "ml_format = \"\"\"\n",
    "<|im_start|>system\n",
    "{system_message}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "print(model)\n",
    "# Define content handler\n",
    "async def content_handler(content: AsyncGenerator[str, None]):\n",
    "    async for token in content:\n",
    "        print(token, end=\"\")\n",
    "\n",
    "formatted_prompt = ml_format.format(system_message=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\", prompt=\"Hello, how are you?\")\n",
    "\n",
    "async def main():\n",
    "    # Request and process stream\n",
    "    resp = openai.Completion.create(\n",
    "        model=model['id'],\n",
    "        prompt= formatted_prompt,\n",
    "        stream=True,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    # await process_response(resp, content_handler)\n",
    "    collected_events = []\n",
    "    completion_text = ''\n",
    "    # iterate through the stream of events\n",
    "    for event in resp:\n",
    "        event_text = event['choices'][0]['text']  # Extract the text\n",
    "        completion_text += event_text  # Append the text\n",
    "        print(f\"\\r{completion_text}\", end='')  # Overwrite the current line with updated text\n",
    "\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name_or_path = \"TheBloke/Dolphin-2.1-70B-AWQ\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=False)\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_quantized(model_name_or_path, fuse_layers=True,\n",
    "                                          trust_remote_code=False, safetensors=True)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''<|im_start|>system\n",
    "{system_message}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "'''\n",
    "\n",
    "print(\"*** Running model.generate:\")\n",
    "\n",
    "token_input = tokenizer(\n",
    "    prompt_template,\n",
    "    return_tensors='pt'\n",
    ").input_ids.cuda()\n",
    "\n",
    "# Generate output\n",
    "generation_output = model.generate(\n",
    "    token_input,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "# Get the tokens from the output, decode them, print them\n",
    "token_output = generation_output[0]\n",
    "text_output = tokenizer.decode(token_output)\n",
    "print(\"LLM output: \", text_output)\n",
    "\n",
    "\"\"\"\n",
    "# Inference should be possible with transformers pipeline as well in future\n",
    "# But currently this is not yet supported by AutoAWQ (correct as of September 25th 2023)\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nest_asyncio if not already done\n",
    "!pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import openai \n",
    "import asyncio\n",
    "\n",
    "openai.api_key = \"EMPTY\"\n",
    "openai.api_base = \"http://127.0.0.1:8100/v1\"\n",
    "\n",
    "async def main():\n",
    "\n",
    "    print(f\"USING MODEL : {model['id']}\")\n",
    "    description = \"Create a Python script to sort a list of numbers in ascending order.\"\n",
    "    response = openai.Completion.create(\n",
    "      model=model[\"id\"],\n",
    "      prompt=f\"Instruction:\\n{description}\",\n",
    "      stream=True,\n",
    "      max_tokens=20\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        for chunk in response:\n",
    "            current_content = chunk[\"choices\"][0][\"delta\"].get(\"content\",\"\")\n",
    "            print(current_content)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while making the request: {e}\")\n",
    "        if e.response is not None:\n",
    "            print(\"Additional information:\")\n",
    "            print(f\"Status code: {e.response.status_code}\")\n",
    "            print(f\"Headers: {e.response.headers}\")\n",
    "            print(f\"Content: {e.response.text}\")\n",
    "\n",
    "# Run the async function\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_numbers(numbers):\n",
    "    return sorted(numbers)\n",
    "\n",
    "# Use example\n",
    "numbers = [67, 23, 34, 2, 89, 23, 2]\n",
    "sorted_numbers = sort_numbers(numbers)\n",
    "print(sorted_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of an OpenAI Completion request, using the stream=True option\n",
    "# https://beta.openai.com/docs/api-reference/completions/create\n",
    "import time\n",
    "# record the time before the request is sent\n",
    "start_time = time.time()\n",
    "\n",
    "# send a Completion request to count to 100\n",
    "response = openai.Completion.create(\n",
    "    model=model['id'],\n",
    "    prompt='1,2,3,',\n",
    "    max_tokens=2000,\n",
    "    temperature=0,\n",
    "    stream=True,  # this time, we set stream=True\n",
    ")\n",
    "\n",
    "# Iterate through the stream of events\n",
    "for event in response:\n",
    "    event_text = event['choices'][0]['text']  # Extract the text from the event\n",
    "    collected_events.append(event)  # Collect the event (optional, if you need it later)\n",
    "    \n",
    "    # Append the streamed text to the existing output\n",
    "    completion_text += event_text\n",
    "    \n",
    "    # Print the updated completion_text\n",
    "    print(completion_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
