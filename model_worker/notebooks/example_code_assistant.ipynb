{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "from api_models import get_models, ModelListResponse,ModelPermission,Model\n",
    "response: ModelListResponse = await get_models()\n",
    "model: Model = response.data[0]\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os \n",
    "import json \n",
    "import requests\n",
    "\n",
    "#Streaming endpoint \n",
    "API_URL = \"http://127.0.0.1:8100/v1/chat/completions\" #os.getenv(\"API_URL\") + \"/generate_stream\"\n",
    "\n",
    "#Inferenec function\n",
    "def predict(system_msg, inputs, top_p, temperature, chat_counter, chatbot=[], history=[]):  \n",
    "\n",
    "    headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer \"\"\"  #Users will provide their own OPENAI_API_KEY \n",
    "    }\n",
    "    print(f\"system message is ^^ {system_msg}\")\n",
    "    if system_msg.strip() == '':\n",
    "        initial_message = [{\"role\": \"user\", \"content\": f\"{inputs}\"},]\n",
    "        multi_turn_message = []\n",
    "    else:\n",
    "        initial_message= [{\"role\": \"system\", \"content\": system_msg},\n",
    "                   {\"role\": \"user\", \"content\": f\"{inputs}\"},]\n",
    "        multi_turn_message = [{\"role\": \"system\", \"content\": system_msg},]\n",
    "        \n",
    "    if chat_counter == 0 :\n",
    "        payload = {\n",
    "        \"model\": model['id'],\n",
    "        \"messages\": initial_message , \n",
    "        \"temperature\" : 1.0,\n",
    "        \"top_p\":1.0,\n",
    "        \"n\" : 1,\n",
    "        \"stream\": True,\n",
    "        \"presence_penalty\":0,\n",
    "        \"frequency_penalty\":0,\n",
    "        }\n",
    "        print(f\"chat_counter - {chat_counter}\")\n",
    "    else: #if chat_counter != 0 :\n",
    "        messages=multi_turn_message # Of the type of - [{\"role\": \"system\", \"content\": system_msg},]\n",
    "        for data in chatbot:\n",
    "          user = {}\n",
    "          user[\"role\"] = \"user\" \n",
    "          user[\"content\"] = data[0] \n",
    "          assistant = {}\n",
    "          assistant[\"role\"] = \"assistant\" \n",
    "          assistant[\"content\"] = data[1]\n",
    "          messages.append(user)\n",
    "          messages.append(assistant)\n",
    "        temp = {}\n",
    "        temp[\"role\"] = \"user\" \n",
    "        temp[\"content\"] = inputs\n",
    "        messages.append(temp)\n",
    "        #messages\n",
    "        payload = {\n",
    "        \"model\": model['id'],\n",
    "        \"messages\": messages, # Of the type of [{\"role\": \"user\", \"content\": f\"{inputs}\"}],\n",
    "        \"temperature\" : temperature, #1.0,\n",
    "        \"top_p\": top_p, #1.0,\n",
    "        \"n\" : 1,\n",
    "        \"stream\": True,\n",
    "        \"presence_penalty\":0,\n",
    "        \"frequency_penalty\":0,}\n",
    "\n",
    "    chat_counter+=1\n",
    "\n",
    "    history.append(inputs)\n",
    "    print(f\"Logging : payload is - {payload}\")\n",
    "    # make a POST request to the API endpoint using the requests.post method, passing in stream=True\n",
    "    response = requests.post(API_URL, headers=headers, json=payload, stream=True)\n",
    "    print(f\"Logging : response code - {response}\")\n",
    "    token_counter = 0 \n",
    "    partial_words = \"\" \n",
    "\n",
    "    counter=0\n",
    "    for chunk in response.iter_lines():\n",
    "        #Skipping first chunk\n",
    "        if counter == 0:\n",
    "          counter+=1\n",
    "          continue\n",
    "        # check whether each line is non-empty\n",
    "        if chunk.decode() :\n",
    "          chunk = chunk.decode()\n",
    "          # decode each line as response data is in bytes\n",
    "          if len(chunk) > 12 and \"content\" in json.loads(chunk[6:])['choices'][0]['delta']:\n",
    "              partial_words = partial_words + json.loads(chunk[6:])['choices'][0][\"delta\"][\"content\"]\n",
    "              if token_counter == 0:\n",
    "                history.append(\" \" + partial_words)\n",
    "              else:\n",
    "                history[-1] = partial_words\n",
    "              chat = [(history[i], history[i + 1]) for i in range(0, len(history) - 1, 2) ]  # convert to tuples of list\n",
    "              token_counter+=1\n",
    "              yield chat, history, chat_counter, response  # resembles {chatbot: chat, state: history}  \n",
    "                   \n",
    "#Resetting to blank\n",
    "def reset_textbox():\n",
    "    return gr.update(value='')\n",
    "\n",
    "#to set a component as visible=False\n",
    "def set_visible_false():\n",
    "    return gr.update(visible=False)\n",
    "\n",
    "#to set a component as visible=True\n",
    "def set_visible_true():\n",
    "    return gr.update(visible=True)\n",
    "\n",
    "#Using info to add additional information about System message in GPT4\n",
    "system_msg_info = \"\"\"You are an AI programming assistant.\n",
    "        \n",
    "                - Follow the user's requirements carefully and to the letter.\n",
    "                - First think step-by-step -- describe your plan for what to build in pseudocode, written out in great detail.\n",
    "                - Then output the code in a single code block.\n",
    "                - Minimize any other prose.\"\"\"\n",
    "      \n",
    "\n",
    "with gr.Blocks(css = \"\"\"#col_container { margin-left: auto; margin-right: auto;} #chatbot {height: 1520px; overflow: auto;}\"\"\",) as demo:\n",
    "    with gr.Column(elem_id = \"col_container\"):\n",
    "        #Users need to provide their own GPT4 API key, it is no longer provided by Huggingface \n",
    "        with gr.Row():\n",
    "            openai_gpt4_key = \"\"\n",
    "            with gr.Accordion(label=\"System message:\", open=False):\n",
    "                system_msg = gr.Textbox(label=\"Instruct the AI Assistant to set its beaviour\", info = system_msg_info, value=\"\",placeholder=\"Type here..\")\n",
    "                accordion_msg = gr.HTML(value=\"ðŸš§ To set System message you will have to refresh the app\", visible=False)\n",
    "                          \n",
    "        chatbot = gr.Chatbot(label='GPT4', elem_id=\"chatbot\")\n",
    "        inputs = gr.Textbox(placeholder= \"Hi there!\", label= \"Type an input and press Enter\")\n",
    "        state = gr.State([]) \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=7):\n",
    "                b1 = gr.Button().style(full_width=True)\n",
    "            with gr.Column(scale=3):\n",
    "                server_status_code = gr.Textbox(label=\"Status code from OpenAI server\", )\n",
    "    \n",
    "        #top_p, temperature\n",
    "        with gr.Accordion(\"Parameters\", open=False):\n",
    "            top_p = gr.Slider( minimum=-0, maximum=1.0, value=1.0, step=0.05, interactive=True, label=\"Top-p (nucleus sampling)\",)\n",
    "            temperature = gr.Slider( minimum=-0, maximum=5.0, value=1.0, step=0.1, interactive=True, label=\"Temperature\",)\n",
    "            chat_counter = gr.Number(value=0, visible=False, precision=0)\n",
    "\n",
    "    #Event handling\n",
    "    inputs.submit( predict, [system_msg, inputs, top_p, temperature, chat_counter, chatbot, state], [chatbot, state, chat_counter, server_status_code],)  #openai_api_key\n",
    "    b1.click( predict, [system_msg, inputs, top_p, temperature, chat_counter, chatbot, state], [chatbot, state, chat_counter, server_status_code],)  #openai_api_key\n",
    "    \n",
    "    inputs.submit(set_visible_false, [], [system_msg])\n",
    "    b1.click(set_visible_false, [], [system_msg])\n",
    "    inputs.submit(set_visible_true, [], [accordion_msg])\n",
    "    b1.click(set_visible_true, [], [accordion_msg])\n",
    "    \n",
    "    b1.click(reset_textbox, [], [inputs])\n",
    "    inputs.submit(reset_textbox, [], [inputs])\n",
    "\n",
    "demo.queue(max_size=99, concurrency_count=20).launch(debug=True)\n",
    "%%javascript\n",
    "%matplotlib inline\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 444em; }</style>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
